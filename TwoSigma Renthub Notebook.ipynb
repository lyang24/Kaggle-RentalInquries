{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Eric Yang\\Anaconda2\\envs\\snakes\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import time\n",
    "\n",
    "from sklearn import preprocessing, pipeline, metrics, model_selection\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import feature_selection\n",
    "from itertools import product\n",
    "\n",
    "import osgeo \n",
    "import fiona\n",
    "import json\n",
    "import geopandas as gpd\n",
    "import geocoder\n",
    "from scipy import sparse\n",
    "from shapely.geometry import Point\n",
    "from geopandas.tools import sjoin\n",
    "from geopy.distance import vincenty\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_json(r'C:\\Users\\Eric Yang\\Desktop\\Kaggle Comptetion\\Current Competition\\train.json')\n",
    "\n",
    "test_data = pd.read_json(r'C:\\Users\\Eric Yang\\Desktop\\Kaggle Comptetion\\Current Competition\\test.json') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "index=list(range(train_data.shape[0]))\n",
    "random.shuffle(index)\n",
    "a=[np.nan]*len(train_data)\n",
    "b=[np.nan]*len(train_data)\n",
    "c=[np.nan]*len(train_data)\n",
    "\n",
    "for i in range(5):\n",
    "    building_level={}\n",
    "    for j in train_data['manager_id'].values:\n",
    "        building_level[j]=[0,0,0]\n",
    "    test_index=index[int((i*train_data.shape[0])/5):int(((i+1)*train_data.shape[0])/5)]\n",
    "    train_index=list(set(index).difference(test_index))\n",
    "    for j in train_index:\n",
    "        temp=train_data.iloc[j]\n",
    "        if temp['interest_level']=='low':\n",
    "            building_level[temp['manager_id']][0]+=1\n",
    "        if temp['interest_level']=='medium':\n",
    "            building_level[temp['manager_id']][1]+=1\n",
    "        if temp['interest_level']=='high':\n",
    "            building_level[temp['manager_id']][2]+=1\n",
    "    for j in test_index:\n",
    "        temp=train_data.iloc[j]\n",
    "        if sum(building_level[temp['manager_id']])!=0:\n",
    "            a[j]=building_level[temp['manager_id']][0]*1.0/sum(building_level[temp['manager_id']])\n",
    "            b[j]=building_level[temp['manager_id']][1]*1.0/sum(building_level[temp['manager_id']])\n",
    "            c[j]=building_level[temp['manager_id']][2]*1.0/sum(building_level[temp['manager_id']])\n",
    "train_data['manager_level_low']=a\n",
    "train_data['manager_level_medium']=b\n",
    "train_data['manager_level_high']=c\n",
    "\n",
    "\n",
    "\n",
    "a=[]\n",
    "b=[]\n",
    "c=[]\n",
    "building_level={}\n",
    "for j in train_data['manager_id'].values:\n",
    "    building_level[j]=[0,0,0]\n",
    "for j in range(train_data.shape[0]):\n",
    "    temp=train_data.iloc[j]\n",
    "    if temp['interest_level']=='low':\n",
    "        building_level[temp['manager_id']][0]+=1\n",
    "    if temp['interest_level']=='medium':\n",
    "        building_level[temp['manager_id']][1]+=1\n",
    "    if temp['interest_level']=='high':\n",
    "        building_level[temp['manager_id']][2]+=1\n",
    "\n",
    "for i in test_data['manager_id'].values:\n",
    "    if i not in building_level.keys():\n",
    "        a.append(np.nan)\n",
    "        b.append(np.nan)\n",
    "        c.append(np.nan)\n",
    "    else:\n",
    "        a.append(building_level[i][0]*1.0/sum(building_level[i]))\n",
    "        b.append(building_level[i][1]*1.0/sum(building_level[i]))\n",
    "        c.append(building_level[i][2]*1.0/sum(building_level[i]))\n",
    "test_data['manager_level_low']=a\n",
    "test_data['manager_level_medium']=b\n",
    "test_data['manager_level_high']=c\n",
    "\n",
    "manager_variable = ['manager_level_low','manager_level_medium','manager_level_high']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_size = train_data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Create target variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_data['target'] = train_data['interest_level'].apply(lambda x: 0 if x=='low' else 1 if x=='medium' else 2)\n",
    "train_data['low'] = train_data['interest_level'].apply(lambda x: 1 if x=='low' else 0)\n",
    "train_data['medium'] = train_data['interest_level'].apply(lambda x: 1 if x=='medium' else 0)\n",
    "train_data['high'] = train_data['interest_level'].apply(lambda x: 1 if x=='high' else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Merge training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "full_data=pd.concat([train_data\n",
    "                       ,test_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "full_data.ix[4620, 'price']  = 1025\n",
    "full_data.ix[12168, 'price']  = 3400\n",
    "full_data.ix[32611, 'price']  = 10000\n",
    "\n",
    "full_data.ix[51229,'latitude'] = 40.831835\n",
    "full_data.ix[51229,'longitude'] = -73.921021\n",
    "full_data.ix[113035, 'latitude'] = 40.869699\n",
    "full_data.ix[113035,'longitude'] = -73.243134\n",
    "full_data.ix[104822, 'latitude'] = 40.752816\n",
    "full_data.ix[104822,'longitude'] = -73.970854\n",
    "full_data.ix[72896, 'latitude'] = 40.772661\n",
    "full_data.ix[72896,'longitude'] = -73.955558\n",
    "full_data.ix[78568, 'latitude'] = 40.763670\n",
    "full_data.ix[78568,'longitude'] = -73.958609\n",
    "full_data.ix[109135, 'latitude'] = 40.778075\n",
    "full_data.ix[109135,'longitude'] = -73.952238\n",
    "full_data.ix[17772, 'latitude'] = 40.748368\n",
    "full_data.ix[17772,'longitude'] = -73.976599\n",
    "full_data.ix[67902, 'latitude'] = 40.805668\n",
    "full_data.ix[67902,'longitude'] = -73.941987\n",
    "full_data.ix[21168, 'latitude'] = 40.731960\n",
    "full_data.ix[21168,'longitude'] = -74.002091\n",
    "full_data.ix[55585, 'latitude'] = 40.737547\n",
    "full_data.ix[55585,'longitude'] = -73.984084\n",
    "full_data.ix[45416, 'latitude'] = 40.754539\n",
    "full_data.ix[45416,'longitude'] = -73.998372\n",
    "full_data.ix[17772, 'latitude'] = 40.747632\n",
    "full_data.ix[17772,'longitude'] = -73.974814\n",
    "full_data.ix[109135, 'latitude'] = 40.775065\n",
    "full_data.ix[109135,'longitude'] = -73.948073\n",
    "full_data.ix[39798, 'latitude' ] = 40.747632\n",
    "full_data.ix[39798,'longitude' ] = -73.974814"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "missing_data = pd.read_csv(r'C:\\Users\\Eric Yang\\Desktop\\Kaggle Comptetion\\Current Competition\\a_few_missing2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "full_data.fillna(0, inplace = True)\n",
    "full_data.loc[(full_data.longitude == 0) | (full_data.latitude == 0), 'latitude'] = missing_data.lat.values\n",
    "full_data.loc[(full_data.longitude == 0) | (full_data.latitude == 0), 'longitude'] = missing_data.long.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Geo Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "missingCoords = full_data[(full_data.longitude == 0) | (full_data.latitude == 0)]\n",
    "missingGeoms = (missingCoords.street_address + ', New York').apply(geocoder.google)\n",
    "\n",
    "full_data.loc[(full_data.longitude == 0) | (full_data.latitude == 0), 'latitude'] = missingGeoms.apply(lambda x: x.lat)\n",
    "full_data.loc[(full_data.longitude == 0) | (full_data.latitude == 0), 'longitude'] = missingGeoms.apply(lambda x: x.lng)\n",
    "\n",
    "missing_data = pd.DataFrame({'lat':missingGeoms.apply(lambda x: x.lat), 'long':missingGeoms.apply(lambda x: x.lng)})\n",
    "missing_data.to_csv(r'C:\\Users\\Eric Yang\\Desktop\\Kaggle Comptetion\\Current Competition\\a_few_missing2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "full_data['geometry'] = full_data.apply(lambda x: Point((float(x.longitude), float(x.latitude))), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "poly = gpd.GeoDataFrame.from_file(r'C:\\Users\\Eric Yang\\Desktop\\Kaggle Comptetion\\Current Competition\\nynta.geojson')\n",
    "gdat = gpd.GeoDataFrame(full_data, crs = poly.crs, geometry='geometry')\n",
    "geo_data = sjoin(gdat,poly, how='left', op='within')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "geo_var = ['boroname','borocode','ntaname']\n",
    "full_data = pd.merge(left = full_data, right = geo_data[geo_var],left_index=True,right_index=True, how = 'left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Group Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "num_vars = ['bathrooms','bedrooms','latitude','longitude','price']\n",
    "cat_vars = ['building_id','manager_id','display_address','street_address', 'borocode','ntaname']\n",
    "text_vars = ['description','features']\n",
    "date_var = 'created'\n",
    "image_var = 'photos'\n",
    "id_var = 'listing_id'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Date variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "full_data['created_datetime'] = pd.to_datetime(full_data['created'], format=\"%Y-%m-%d %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# full_data['created_year']=full_data['created_datetime'].apply(lambda x:x.year) ## low variant\n",
    "full_data['created_month']=full_data['created_datetime'].apply(lambda x:x.month)\n",
    "full_data['created_day']=full_data['created_datetime'].apply(lambda x:x.day)\n",
    "full_data['created_dayofweek']=full_data['created_datetime'].apply(lambda x:x.dayofweek)\n",
    "full_data['created_dayofyear']=full_data['created_datetime'].apply(lambda x:x.dayofyear)\n",
    "full_data['created_weekofyear']=full_data['created_datetime'].apply(lambda x:x.weekofyear)\n",
    "full_data['created_hour']=full_data['created_datetime'].apply(lambda x:x.hour)\n",
    "full_data['created_epoch']=full_data['created_datetime'].apply(lambda x:x.value//10**9)\n",
    "\n",
    "date_num_vars = ['created_month','created_dayofweek','created_dayofyear'\n",
    "                 ,'created_weekofyear','created_hour','created_epoch']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Additional Numeric Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# full_data['price']=full_data['price'].apply(np.log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "full_data['rooms'] = full_data['bedrooms'] + full_data['bathrooms'] \n",
    "full_data['num_of_photos'] = full_data['photos'].apply(lambda x:len(x))\n",
    "full_data['num_of_features'] = full_data['features'].apply(lambda x:len(x))\n",
    "full_data['len_of_desc'] = full_data['description'].apply(lambda x:len(x))\n",
    "full_data['words_of_desc'] = full_data['description'].apply(lambda x:len(re.sub('['+string.punctuation+']', '', x).split()))\n",
    "\n",
    "\n",
    "full_data['nums_of_desc'] = full_data['description']\\\n",
    "        .apply(lambda x:re.sub('['+string.punctuation+']', '', x).split())\\\n",
    "        .apply(lambda x: len([s for s in x if s.isdigit()]))\n",
    "        \n",
    "full_data['has_phone'] = full_data['description'].apply(lambda x:re.sub('['+string.punctuation+']', '', x).split())\\\n",
    "        .apply(lambda x: [s for s in x if s.isdigit()])\\\n",
    "        .apply(lambda x: len([s for s in x if len(str(s))==10]))\\\n",
    "        .apply(lambda x: 1 if x>0 else 0)\n",
    "full_data['has_email'] = full_data['description'].apply(lambda x: 1 if '@renthop.com' in x else 0)\n",
    "full_data['price_latitue'] = (full_data[\"price\"])/ (full_data[\"latitude\"]+1.0) \n",
    "full_data['price_longtitude'] =  (full_data[\"price\"])/ (full_data[\"longitude\"]-1.0) \n",
    "\n",
    "additional_num_vars = ['rooms','num_of_photos','num_of_features','len_of_desc',\n",
    "                    'words_of_desc','has_phone','has_email','price_latitue','price_longtitude']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Numeric interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "full_data['avg_word_len'] = full_data[['len_of_desc','words_of_desc']]\\\n",
    "                                    .apply(lambda x: x[0]/x[1] if x[1]!=0 else 0, axis=1)\n",
    "    \n",
    "full_data['price_per_room'] = full_data[['price','rooms']].apply(lambda x: x[0]/x[1] if x[1]!=0 else 0, axis=1)\n",
    "full_data['price_per_bedroom'] = full_data[['price','bedrooms']].apply(lambda x: x[0]/x[1] if x[1]!=0 else 0, axis=1)\n",
    "full_data['price_per_bathroom'] = full_data[['price','bathrooms']].apply(lambda x: x[0]/x[1] if x[1]!=0 else 0, axis=1)\n",
    "full_data['price_per_photo'] = full_data[['price','num_of_photos']].apply(lambda x: x[0]/x[1] if x[1]!=0 else 0, axis=1)\n",
    "\n",
    "\n",
    "full_data['photos_per_room'] = full_data[['num_of_photos','rooms']].apply(lambda x: x[0]/x[1] if x[1]!=0 else 0, axis=1)\n",
    "\n",
    "interactive_num_vars = ['avg_word_len','price_per_room','price_per_bedroom','price_per_bathroom','price_per_photo',\n",
    "                        'photos_per_room']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Categorical Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 1. Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "full_data.borocode.fillna(6, inplace = True)\n",
    "full_data.ntaname.fillna('other', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Encoding building_id\n",
      "Label Encoding manager_id\n",
      "Label Encoding display_address\n",
      "Label Encoding street_address\n",
      "Label Encoding borocode\n",
      "Label Encoding ntaname\n",
      "Label-encoded feaures: ['building_id_le', 'manager_id_le', 'display_address_le', 'street_address_le', 'borocode_le', 'ntaname_le']\n"
     ]
    }
   ],
   "source": [
    "LBL = preprocessing.LabelEncoder()\n",
    "\n",
    "LE_vars=[]\n",
    "LE_map=dict()\n",
    "for cat_var in cat_vars:\n",
    "    print (\"Label Encoding %s\" % (cat_var))\n",
    "    LE_var=cat_var+'_le'\n",
    "    full_data[LE_var]=LBL.fit_transform(full_data[cat_var])\n",
    "    LE_vars.append(LE_var)\n",
    "    LE_map[cat_var]=LBL.classes_\n",
    "    \n",
    "print (\"Label-encoded feaures: %s\" % (LE_vars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "LE_vars.remove('ntaname_le')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 2. One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-hot-encoding finished in 0.301259 seconds\n",
      "OHE_sparse size : (124011, 57874)\n",
      "One-hot encoded catgorical feature samples : ['building_0', 'building_00005cb939f9986300d987652c933e15', 'building_00024d77a43f0606f926e2312513845c', 'building_000ae4b7db298401cdae2b0ba1ea8146', 'building_0012f1955391bca600ec301035b97b65', 'building_0021440c04241281a436ec21accc40b1', 'building_002d1eba40aa0a6610e04ff20543585f', 'building_003d8740e21484dcc2280639b25539a4', 'building_00480e54b53fe77d17964be3f8307a99', 'building_00553d95d22484bcc36831c9248d1dbc', 'building_0055c8662ba19e95f78df97592d2b83e', 'building_0056dbdf2881b76f2a0171eb753ec9e0', 'building_0059ae562b9e338a59eaf962cb3eedd2', 'building_005e0f8d7fb7b92be351cbf1dd985149', 'building_0067f166111490e7af7f1a878a67bb5e', 'building_0070bc94a3f80aa717bb15708e98ba54', 'building_0071cda335745940cdae1dc31abfc701', 'building_0078281cd69f4bfec17e42e5cf5eecd9', 'building_0078c2ab46afba9969637ac83621901e', 'building_007ae1cd90420f18bad7b6892a9a1411', 'building_007cd8edc45c6cfbcabd88f70d59a513', 'building_008d3e3a11295305966844713b685f7d', 'building_008ff72d77a8fc85eccfc4ec33ec09a3', 'building_0095cb49c423ec7b204e26d76c56bd35', 'building_009c6ad006e8fd679991c5f8cffaef9f', 'building_009f494b0636f32b96b41926ec7c4bf2', 'building_00a4e18de6c9a7bbec33c77e0588a3b9', 'building_00a61b88186b5115356374b0f5dd0d1e', 'building_00a7b4a6aec7ca1a1635c622918b68f0', 'building_00a94a38fcda000b4448370839a25ac8', 'building_00b2da856a75f0f5690996b0a0b1f397', 'building_00bafd8e05682a7c7e36b4046acd0f1b', 'building_00bb734cde488aa3e1f3e5f1376b9c13', 'building_00cca782a37fc2bb91b080ede56fa7cf', 'building_00ce59a4de554163bda36549de6bf967', 'building_00d1b109f921cd8bc69a203bf35a9bac', 'building_00dfd2bccb9127f2e7966ff29ae1e060', 'building_00e3e6bb0d19bb601842c0ab9589f9a2', 'building_00e8bbc4c74980a06c187165d9a5869e', 'building_00e980b7c97376eb19e0b1be650ccb64', 'building_00ecc203c49a4651cf186de65f308ac1', 'building_00fdda7f129d05ab200c31c0c6de8dfd', 'building_00ffdfd150acc0b097182bbf9dd1db28', 'building_0103d0d57f197a73cdfd0f2f26870d74', 'building_010435ab3b0b415421d583937a55283e', 'building_0106f282d2dfe616303b86e5b68df6b4', 'building_010f3d0141cd76667ca8e3d86e221cf2', 'building_0114c80bf2a9027612083e354d7fbdbc', 'building_0117976b081298aea99e21c327ab25a4', 'building_011a3c781df937c2991a3832b547b158', 'building_01298c8fd1e6e332fb4c188a7183a206', 'building_012fcf2833e0e07897b682ab6f82be3b', 'building_013a96b772f0e46731faee50ad25d727', 'building_01401bc9a8908b2d6ffa84ebf9e1b984', 'building_0145e758b990b8d2648ee57c30762d76', 'building_0152c6255a4e29051b817ce6f3f6dd6f', 'building_0155fa9629c4d68dbe9106b9ac3866ad', 'building_015901f6966b695aec0bda32e36423d6', 'building_015bf4a41140bb7304d338437192f2ab', 'building_015cf622b143eb8703307cf591e9dd46', 'building_01601e509316d5e48176d0f034e04f9f', 'building_016124e48651db465762bfbaf7a9080b', 'building_0162edae269da1472fcbdf4f61df7c7a', 'building_0165775b1775642fb0f71595e83484ba', 'building_0166aa8add8bdc29ca360ab1db5c77f9', 'building_016e30568187a4bd83bf86ca02837544', 'building_017c4715165fe36e5a4372ebf15207f1', 'building_017c9eb62166b050a144adad22b91c00', 'building_017e4ac152c10d537fb2acb3743f2162', 'building_018023115a3d3d3273b25a3357e5358a', 'building_0180389cfd65c7037149466dc934afd3', 'building_01813414c43aedaf8f1b1fb74cef3f99', 'building_018b0a85428622d9a6447fd475cf8bff', 'building_018cd3169a68bd83a92c341d8349b77d', 'building_01904e878eec9c80a40d70f6495454c2', 'building_0195041256b2514a6047a340b0561c3e', 'building_019adbb79eb99ed4c4b9dea91571aa91', 'building_019ce6c2904c70141b3c6fd6d7557344', 'building_019e909a65d7d2e3bf22c6334bf15a7e', 'building_01a430b383a468bcfb6a6dec9f6d42cd', 'building_01a8c78a8ec0234b5e9bd8645fb2ce43', 'building_01acba2c701ccf3528ddf17f1006f694', 'building_01ae28a1834d9bb940cfa2daa78b59c6', 'building_01bb059f7b7619ed7a5f74a97b9ce2ed', 'building_01bd855b1c9b786e37c13c25e5a81fda', 'building_01bfd6631a820706cb16bae93585d998', 'building_01cb79b2d06a9844231c42b3f05eb0c2', 'building_01cb7ce5bc25ca98fcca6ed5236c3224', 'building_01cd1cd0b21bbd22da7381a248d4fdc8', 'building_01cefee671bc17927058c0c579d459cb', 'building_01d767115586534b39d5224710dc56a4', 'building_01d79491cfd8271eaf73a5d69a759ef0', 'building_01d7a001b3a497c25c3a01955d45c339', 'building_01db3503f72909222100231ff6905a78', 'building_01df496b33f2402c5c18076b5f45c916', 'building_01e23bdc82cbdb20a15d290c02572a5d', 'building_01ee5fb703946538f9a34e22f48939e9', 'building_01f27b3de1bf1ee3218e88e64c3315a0', 'building_01f582fe206ca52144a3ac43c14653f7', 'building_01f85713461b5be6a172f4c3db190668']\n"
     ]
    }
   ],
   "source": [
    "OHE = preprocessing.OneHotEncoder(sparse=True)\n",
    "start=time.time()\n",
    "OHE.fit(full_data[LE_vars])\n",
    "OHE_sparse=OHE.transform(full_data[LE_vars])\n",
    "                                   \n",
    "print ('One-hot-encoding finished in %f seconds' % (time.time()-start))\n",
    "\n",
    "\n",
    "OHE_vars = [var[:-3] + '_' + str(level).replace(' ','_')\\\n",
    "                for var in cat_vars for level in LE_map[var] ]\n",
    "\n",
    "print (\"OHE_sparse size :\" ,OHE_sparse.shape)\n",
    "print (\"One-hot encoded catgorical feature samples : %s\" % (OHE_vars[:100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 3. Leave-one-out Encoding\n",
    "\n",
    "Based on the paper \"A Preprocessing Scheme for High-Cardinality Categorical Attributes in Classification and Prediction Problems\"\n",
    "\n",
    "http://helios.mm.di.uoa.gr/~rouvas/ssi/sigkdd/sigkdd.vol3.1/barreca.ps\n",
    "\n",
    "** A couple of Kaggle scripts: **\n",
    "\n",
    "R version: by Braden Murray: https://www.kaggle.com/brandenkmurray/two-sigma-connect-rental-listing-inquiries/it-is-lit/comments\n",
    "\n",
    "Python Version 1, by Stanislav Ushakov\n",
    "https://www.kaggle.com/stanislavushakov/two-sigma-connect-rental-listing-inquiries/python-version-of-it-is-lit-by-branden/comments\n",
    "\n",
    "Python Version 2, by Rakhlin\n",
    "https://www.kaggle.com/rakhlin/two-sigma-connect-rental-listing-inquiries/another-python-version-of-it-is-lit-by-branden/code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "##Create a function to encode high-cardinality cateogrical features\n",
    "\n",
    "def designate_single_observations(df1, df2, column):\n",
    "    ps = df1[column].append(df2[column])\n",
    "    grouped = ps.groupby(ps).size().to_frame().rename(columns={0: \"size\"})\n",
    "    df1.loc[df1.join(grouped, on=column, how=\"left\")[\"size\"] <= 1, column] = -1\n",
    "    df2.loc[df2.join(grouped, on=column, how=\"left\")[\"size\"] <= 1, column] = -1\n",
    "    return df1, df2\n",
    "\n",
    "\n",
    "def hcc_encode(train_df, test_df, variable, target, prior_prob, k, f=1, g=1, r_k=None, update_df=None):\n",
    "    \"\"\"\n",
    "    See \"A Preprocessing Scheme for High-Cardinality Categorical Attributes in\n",
    "    Classification and Prediction Problems\" by Daniele Micci-Barreca\n",
    "    \"\"\"\n",
    "    hcc_name = \"_\".join([\"hcc\", variable, target])\n",
    "\n",
    "    grouped = train_df.groupby(variable)[target].agg({\"size\": \"size\", \"mean\": \"mean\"})\n",
    "    grouped[\"lambda\"] = 1 / (g + np.exp((k - grouped[\"size\"]) / f))\n",
    "    grouped[hcc_name] = grouped[\"lambda\"] * grouped[\"mean\"] + (1 - grouped[\"lambda\"]) * prior_prob\n",
    "\n",
    "    df = test_df[[variable]].join(grouped, on=variable, how=\"left\")[hcc_name].fillna(prior_prob)\n",
    "    if r_k: df *= np.random.uniform(1 - r_k, 1 + r_k, len(test_df))     # Add uniform noise. Not mentioned in original paper\n",
    "\n",
    "    if update_df is None: update_df = test_df\n",
    "    if hcc_name not in update_df.columns: update_df[hcc_name] = np.nan\n",
    "    update_df.update(df)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for col in ('building_id', 'manager_id', 'display_address'):\n",
    "    train_data, test_data = designate_single_observations(train_data, test_data, col)\n",
    "    \n",
    "prior_low, prior_medium, prior_high = train_data[[\"low\", \"medium\", \"high\"]].mean() \n",
    "\n",
    "skf = model_selection.StratifiedKFold(5)\n",
    "attributes = product((\"building_id\", \"manager_id\"), zip((\"medium\", \"high\"), (prior_medium, prior_high)))\n",
    "for variable, (target, prior) in attributes:\n",
    "    hcc_encode(train_data, test_data, variable, target, prior, k=5, r_k=None)\n",
    "    for train, test in skf.split(np.zeros(len(train_data)), train_data['interest_level']):\n",
    "        hcc_encode(train_data.iloc[train], train_data.iloc[test], variable, target, prior, k=5, r_k=0.01,\n",
    "                   update_df=train_data)\n",
    "        \n",
    "hcc_data = pd.concat([train_data[['building_id', 'manager_id', 'display_address',\n",
    "            'hcc_building_id_medium','hcc_building_id_high',\n",
    "            'hcc_manager_id_medium','hcc_manager_id_high']],\n",
    "           test_data[['building_id', 'manager_id', 'display_address',\n",
    "            'hcc_building_id_medium','hcc_building_id_high',\n",
    "            'hcc_manager_id_medium','hcc_manager_id_high']]\n",
    "           ]\n",
    "          )\n",
    "full_data['building_id'] = hcc_data['building_id']\n",
    "full_data['manager_id'] = hcc_data['manager_id']\n",
    "full_data['display_address'] = hcc_data['display_address']\n",
    "full_data['hcc_building_id_medium'] = hcc_data['hcc_building_id_medium']\n",
    "full_data['hcc_building_id_high'] = hcc_data['hcc_building_id_high']\n",
    "full_data['hcc_manager_id_medium'] = hcc_data['hcc_manager_id_medium']\n",
    "full_data['hcc_manager_id_high'] = hcc_data['hcc_manager_id_high']\n",
    "hcc_vars = ['hcc_building_id_medium','hcc_building_id_high','hcc_manager_id_medium','hcc_manager_id_high']    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['bathrooms', 'bedrooms', 'building_id', 'created', 'description',\n",
       "       'display_address', 'features', 'high', 'interest_level', 'latitude',\n",
       "       'listing_id', 'longitude', 'low', 'manager_id', 'manager_level_high',\n",
       "       'manager_level_low', 'manager_level_medium', 'medium', 'photos',\n",
       "       'price', 'street_address', 'target', 'geometry', 'boroname', 'borocode',\n",
       "       'ntaname', 'created_datetime', 'created_month', 'created_day',\n",
       "       'created_dayofweek', 'created_dayofyear', 'created_weekofyear',\n",
       "       'created_hour', 'created_epoch', 'rooms', 'num_of_photos',\n",
       "       'num_of_features', 'len_of_desc', 'words_of_desc', 'nums_of_desc',\n",
       "       'has_phone', 'has_email', 'price_latitue', 'price_longtitude',\n",
       "       'avg_word_len', 'price_per_room', 'price_per_bedroom',\n",
       "       'price_per_bathroom', 'price_per_photo', 'photos_per_room',\n",
       "       'building_id_le', 'manager_id_le', 'display_address_le',\n",
       "       'street_address_le', 'borocode_le', 'ntaname_le',\n",
       "       'hcc_building_id_medium', 'hcc_building_id_high',\n",
       "       'hcc_manager_id_medium', 'hcc_manager_id_high'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "scale = preprocessing.StandardScaler()\n",
    "cluster_vars = ['created_weekofyear','latitude', 'longitude','ntaname_le', 'bedrooms']\n",
    "cluster_df = full_data[cluster_vars]\n",
    "cluster_df['bedrooms'] = cluster_df['bedrooms'].clip_upper(5)\n",
    "cluster_df['bedrooms'] = cluster_df['bedrooms'].map(lambda x: 0.7 * x)\n",
    "cluster_df['created_weekofyear'] = cluster_df['created_weekofyear'].map(lambda x: 6 * x)\n",
    "cluster_df[cluster_vars] = scale.fit_transform(cluster_df[cluster_vars])\n",
    "db = DBSCAN(eps=0.28, min_samples=25).fit(cluster_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "labels = db.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "label_set = set(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -1,   0,   1, ...,  52,  17, 110], dtype=int64)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "full_data['clusters'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40.732916676970639, -74.001560661128295)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(full_data[full_data['clusters'] == 1].latitude.sum(axis = 0)/len(full_data[full_data['clusters'] == 1])\\\n",
    "    ,full_data[full_data['clusters'] == 1].longitude.sum(axis = 0)/len(full_data[full_data['clusters'] == 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "centroids = []\n",
    "for i in label_set:\n",
    "   centroids.append((full_data[full_data['clusters'] == i].latitude.sum(axis = 0)/len(full_data[full_data['clusters'] == i]),\\\n",
    "                    full_data[full_data['clusters'] == i].longitude.sum(axis = 0)/len(full_data[full_data['clusters'] == i])))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'geo' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-8048dbfa0445>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfull_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mgeo\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'geo' is not defined"
     ]
    }
   ],
   "source": [
    "np.asarray(full_data[geo])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "centroids = np.asarray(centroids).reshape(-1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "geo = ['latitude', 'longitude']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "geo = ['latitude', 'longitude']\n",
    "full_data['coords'] = list(zip(full_data.latitude, full_data.longitude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "full_data['coords'].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "vincenty(full_data['coords'].head(1), centroids[0]).meters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def distance(x):\n",
    "    global centroids\n",
    "    global label_set\n",
    "    dist = []\n",
    "    for label in label_set:\n",
    "        df = full_data[full_data['coords'] == label]\n",
    "        for cent in centroids:\n",
    "                vincenty(x, cent).meters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "k_euclid_distance = [distance.cdist(np.asarray(full_data[geo]), cent, 'euclidean') for cent in centroids] #each point to each cluster center\n",
    "dist = [np.min(ke,axis = 1) for ke in k_euclid_distance]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "full_data[full_data['clusters'] == 11].ntaname_le.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Text Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 1. Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "full_data[\"features\"].apply(lambda x: \" \".join([\"_\".join(i.split(\" \")) for i in x]))\n",
    "cntvec = CountVectorizer(stop_words='english', max_features=200)\n",
    "feature_sparse =cntvec.fit_transform(full_data[\"features\"]\\\n",
    "                                     .apply(lambda x: \" \".join([\"_\".join(i.split(\" \")) for i in x])))\n",
    "\n",
    "feature_vars = ['feature_' + v for v in cntvec.vocabulary_]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 2. Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### tf-idf not working, instead, using CountVectorizer, not really helping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# tfidf = TfidfVectorizer(stop_words='english', max_features=10)\n",
    "# desc_sparse = tfidf.fit_transform(full_data[\"description\"])\n",
    "# desc_vars = ['desc_' + v for v in tfidf.get_feature_names()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "cntvec = CountVectorizer(stop_words='english', max_features=100)\n",
    "desc_sparse = cntvec.fit_transform(full_data[\"description\"])\n",
    "desc_vars = ['desc_' + v for v in cntvec.vocabulary_]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### word2vec - to be added"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Street Address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# embedding\n",
    "\n",
    "cntvec = CountVectorizer(stop_words='english', max_features=10)\n",
    "st_addr_sparse = cntvec.fit_transform(full_data[\"street_address\"])\n",
    "st_addr_vars = ['desc_' + v for v in cntvec.vocabulary_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['bathrooms', 'bedrooms', 'building_id', 'created', 'description',\n",
       "       'display_address', 'features', 'high', 'interest_level', 'latitude',\n",
       "       'listing_id', 'longitude', 'low', 'manager_id', 'manager_level_high',\n",
       "       'manager_level_low', 'manager_level_medium', 'medium', 'photos',\n",
       "       'price', 'street_address', 'target', 'geometry', 'boroname', 'borocode',\n",
       "       'ntaname', 'created_datetime', 'created_month', 'created_day',\n",
       "       'created_dayofweek', 'created_dayofyear', 'created_weekofyear',\n",
       "       'created_hour', 'created_epoch', 'rooms', 'num_of_photos',\n",
       "       'num_of_features', 'len_of_desc', 'words_of_desc', 'nums_of_desc',\n",
       "       'has_phone', 'has_email', 'price_latitue', 'price_longtitude',\n",
       "       'avg_word_len', 'price_per_room', 'price_per_bedroom',\n",
       "       'price_per_bathroom', 'price_per_photo', 'photos_per_room',\n",
       "       'building_id_le', 'manager_id_le', 'display_address_le',\n",
       "       'street_address_le', 'borocode_le', 'ntaname_le',\n",
       "       'hcc_building_id_medium', 'hcc_building_id_high',\n",
       "       'hcc_manager_id_medium', 'hcc_manager_id_high', 'clusters'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Numberic vs Categorical Interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "price_by_manager = full_data.groupby('manager_id')['price'].agg([np.min,np.max,np.median,np.mean]).reset_index()\n",
    "price_by_manager.columns = ['manager_id','min_price_by_manager',\n",
    "                            'max_price_by_manager','median_price_by_manager','mean_price_by_manager']\n",
    "full_data = pd.merge(full_data,price_by_manager, how='left',on='manager_id')\n",
    "\n",
    "created_epoch_by_manager = full_data.groupby('manager_id')['created_epoch'].agg([np.min,np.max,np.median,np.mean]).reset_index()\n",
    "created_epoch_by_manager.columns = ['manager_id','min_created_epoch_by_manager',\n",
    "                            'max_created_epoch_by_manager','median_created_epoch_by_manager','mean_created_epoch_by_manager']\n",
    "full_data = pd.merge(full_data,created_epoch_by_manager, how='left',on='manager_id')\n",
    "\n",
    "\n",
    "price_by_building = full_data.groupby('building_id')['price'].agg([np.min,np.max,np.median,np.mean]).reset_index()\n",
    "price_by_building.columns = ['building_id','min_price_by_building',\n",
    "                            'max_price_by_building','median_price_by_building','mean_price_by_building']\n",
    "full_data = pd.merge(full_data,price_by_building, how='left',on='building_id')\n",
    "\n",
    "\n",
    "created_epoch_by_building = full_data.groupby('building_id')['created_epoch'].agg([np.min,np.max,np.median,np.mean]).reset_index()\n",
    "price_by_building.columns = ['building_id','min_created_epoch_by_building',\n",
    "                            'max_created_epoch_by_building','median_created_epoch_by_building','mean_created_epoch_by_building']\n",
    "full_data = pd.merge(full_data,price_by_building, how='left',on='building_id')\n",
    "\n",
    "price_by_disp_addr = full_data.groupby('display_address')['price'].agg([np.min,np.max,np.median,np.mean]).reset_index()\n",
    "price_by_disp_addr.columns = ['display_address','min_price_by_disp_addr',\n",
    "                            'max_price_by_disp_addr','median_price_by_disp_addr','mean_price_by_disp_addr']\n",
    "full_data = pd.merge(full_data,price_by_disp_addr, how='left',on='display_address')\n",
    "\n",
    "# remove one\n",
    "\n",
    "\n",
    "full_data['price_percentile_by_manager']=\\\n",
    "            full_data[['price','min_price_by_manager','max_price_by_manager']]\\\n",
    "            .apply(lambda x:(x[0]-x[1])/(x[2]-x[1]) if (x[2]-x[1])!=0 else 0.5,\n",
    "                  axis=1)\n",
    "full_data['price_percentile_by_building']=\\\n",
    "            full_data[['price','min_price_by_building','max_price_by_building']]\\\n",
    "            .apply(lambda x:(x[0]-x[1])/(x[2]-x[1]) if (x[2]-x[1])!=0 else 0.5,\n",
    "                  axis=1)\n",
    "full_data['price_percentile_by_disp_addr']=\\\n",
    "            full_data[['price','min_price_by_disp_addr','max_price_by_disp_addr']]\\\n",
    "            .apply(lambda x:(x[0]-x[1])/(x[2]-x[1]) if (x[2]-x[1])!=0 else 0.5,\n",
    "                  axis=1)\n",
    "\n",
    "\n",
    "full_data['created_epoch_percentile_by_manager']=\\\n",
    "            full_data[['created_epoch','min_created_epoch_by_manager','max_created_epoch_by_manager']]\\\n",
    "            .apply(lambda x:(x[0]-x[1])/(x[2]-x[1]) if (x[2]-x[1])!=0 else 0.5,\n",
    "                  axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "price_by_cluster = full_data.groupby('clusters')['price'].agg([np.min,np.max,np.median,np.mean]).reset_index()\n",
    "price_by_cluster.columns = ['clusters','min_price_by_clusters',\n",
    "                            'max_price_by_clusters','median_price_by_clusters','mean_price_by_clusters']\n",
    "full_data = pd.merge(full_data,price_by_cluster, how='left',on='clusters')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "full_data['price_percentile_by_clusters']=\\\n",
    "            full_data[['clusters','min_price_by_clusters','max_price_by_clusters']]\\\n",
    "            .apply(lambda x:(x[0]-x[1])/(x[2]-x[1]) if (x[2]-x[1])!=0 else 0.5,\n",
    "                  axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "created_epoch_by_display_address = full_data.groupby('display_address')['created_epoch'].agg([np.min,np.max,np.median,np.mean]).reset_index()\n",
    "created_epoch_by_display_address.columns = ['display_address','min_created_epoch_by_display_address',\n",
    "                            'max_created_epoch_by_display_address','median_created_epoch_by_display_address','mean_created_epoch_by_display_address']\n",
    "full_data = pd.merge(full_data,created_epoch_by_display_address, how='left',on='display_address')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "num_cat_vars = ['median_price_by_manager','mean_price_by_manager',\n",
    "                'median_price_by_building','mean_price_by_building',\n",
    "                'median_price_by_disp_addr','mean_price_by_disp_addr',\n",
    "                'median_created_epoch_by_manager','mean_created_epoch_by_manager',\n",
    "                'price_percentile_by_manager','price_percentile_by_building',\n",
    "                'price_percentile_by_disp_addr','created_epoch_percentile_by_manager',\n",
    "                'median_price_by_clusters', 'mean_price_by_clusters', \n",
    "                'price_percentile_by_clusters','listing_id','listing_id_pos',\n",
    "                'median_created_epoch_by_display_address','mean_created_epoch_by_display_address'\n",
    "               ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Listing ID matters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "min_listing_id = full_data['listing_id'].min()\n",
    "max_listing_id = full_data['listing_id'].max()\n",
    "full_data['listing_id_pos']=full_data['listing_id'].apply(lambda x:np.float64((x-min_listing_id+1))/(max_listing_id-min_listing_id+1))\n",
    "num_vars.append('listing_id')\n",
    "num_vars.append('listing_id_pos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "full_vars = num_vars + date_num_vars + additional_num_vars + interactive_num_vars + LE_vars + hcc_vars + num_cat_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "num_vars = ['bathrooms',\n",
    " 'bedrooms',\n",
    " 'latitude',\n",
    " 'longitude',\n",
    " 'price']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "centroid = (full_data.latitude.sum(axis = 0)/len(full_data)\\\n",
    "    ,full_data.longitude.sum(axis = 0)/len(full_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "geo = ['latitude', 'longitude']\n",
    "full_data['coords'] = list(zip(full_data.latitude, full_data.longitude))\n",
    "def dist_to_center(x):\n",
    "    true_center = np.array((40.7128,-74.0059)).reshape(-1,2)\n",
    "    k = distance.cdist(np.array(x).reshape(-1,2), true_center, 'euclidean') \n",
    "    return k.astype(float)\n",
    "full_data['dist_to_center'] = full_data['coords'].map(dist_to_center)\n",
    "full_data['dist_to_center'] = full_data['dist_to_center'].str[0]\n",
    "full_data['dist_to_center'] = full_data['dist_to_center'].str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "dist_var = ['dist_to_center']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Create training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data size:  (49352, 268) testing data size:  (74659, 268)\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[265]\ttrain-mlogloss:0.320483+0.00173725\ttest-mlogloss:0.529278+0.00641829\n",
      "\n",
      "Wall time: 4min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "##Baseline with features from \"features\" and street address\n",
    "\n",
    "full_vars = num_vars + date_num_vars\\\n",
    "+ additional_num_vars + interactive_num_vars + LE_vars + hcc_vars + num_cat_vars + manager_variable + dist_var\n",
    "\n",
    "train_x = sparse.hstack([full_data[full_vars],feature_sparse,st_addr_sparse]).tocsr()[:train_size]\n",
    "train_y = full_data['target'][:train_size].values\n",
    "\n",
    "test_x = sparse.hstack([full_data[full_vars], feature_sparse,st_addr_sparse]).tocsr()[train_size:]\n",
    "test_y = full_data['target'][train_size:].values\n",
    "\n",
    "\n",
    "full_vars = full_vars + feature_vars + st_addr_vars\n",
    "print (\"training data size: \", train_x.shape,\"testing data size: \", test_x.shape)\n",
    "\n",
    "\n",
    "params = dict()\n",
    "params['objective'] = 'multi:softprob'\n",
    "params['num_class'] = 3\n",
    "params['eta'] = 0.1\n",
    "params['max_depth'] = 6\n",
    "params['min_child_weight'] = 1\n",
    "params['subsample'] = 0.7\n",
    "params['colsample_bytree'] = 0.7\n",
    "params['gamma'] = 1\n",
    "params['seed']=1234\n",
    "\n",
    "cv_results = xgb.cv(params, xgb.DMatrix(train_x, label=train_y.reshape(train_x.shape[0],1)),\n",
    "               num_boost_round=1000000, nfold=5,\n",
    "       metrics={'mlogloss'},\n",
    "       seed=1234,\n",
    "       callbacks=[xgb.callback.early_stop(50)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      feature_name  importance\n",
      "0                             hcc_building_id_high    0.034237\n",
      "1                                price_per_bedroom    0.032046\n",
      "2                           hcc_building_id_medium    0.031733\n",
      "3                              hcc_manager_id_high    0.031451\n",
      "4                                   price_per_room    0.031389\n",
      "5                                            price    0.031013\n",
      "6                      price_percentile_by_manager    0.029918\n",
      "7                    price_percentile_by_disp_addr    0.029104\n",
      "8                                manager_level_low    0.027477\n",
      "9                     price_percentile_by_building    0.027258\n",
      "10                                 price_per_photo    0.027101\n",
      "11                           hcc_manager_id_medium    0.026569\n",
      "12                            manager_level_medium    0.025631\n",
      "13                                    avg_word_len    0.025474\n",
      "14                              display_address_le    0.025443\n",
      "15             created_epoch_percentile_by_manager    0.025036\n",
      "16                              manager_level_high    0.024942\n",
      "17                                  dist_to_center    0.024629\n",
      "18                                  building_id_le    0.024504\n",
      "19                                        latitude    0.022971\n",
      "20                               street_address_le    0.022188\n",
      "21                                       longitude    0.021781\n",
      "22                                      listing_id    0.021656\n",
      "23                                   manager_id_le    0.018933\n",
      "24                                     len_of_desc    0.018589\n",
      "25                                   words_of_desc    0.018120\n",
      "26                              price_per_bathroom    0.017463\n",
      "27                                    created_hour    0.014646\n",
      "28         median_created_epoch_by_display_address    0.014552\n",
      "29                                 num_of_features    0.014458\n",
      "..                                             ...         ...\n",
      "133                      feature_dryer_in_building    0.000063\n",
      "134                              feature_terraces_    0.000063\n",
      "135                             feature_dishwasher    0.000063\n",
      "136                            feature_lounge_room    0.000031\n",
      "137                   feature_decorative_fireplace    0.000031\n",
      "138                             feature_site_super    0.000031\n",
      "139  feature_valet_services_including_dry_cleaning    0.000031\n",
      "140                 feature_site_parking_available    0.000031\n",
      "141                          feature_exposed_brick    0.000031\n",
      "142                                   feature_post    0.000031\n",
      "143                       feature_residents_garden    0.000031\n",
      "144                            feature_unit_washer    0.000031\n",
      "145                          feature_parking_space    0.000031\n",
      "146                                feature_outdoor    0.000031\n",
      "147                                feature_simplex    0.000031\n",
      "148                                   feature_flex    0.000031\n",
      "149                        feature__exposed_brick_    0.000031\n",
      "150                        feature_private_terrace    0.000031\n",
      "151                                  feature_sauna    0.000031\n",
      "152                                   feature_roof    0.000031\n",
      "153                          feature_outdoor_space    0.000031\n",
      "154                                feature__photos    0.000031\n",
      "155                                  feature_multi    0.000031\n",
      "156                                  feature_space    0.000031\n",
      "157                            feature_wifi_access    0.000031\n",
      "158                       feature_pets_on_approval    0.000031\n",
      "159                           feature__lndry_bldg_    0.000031\n",
      "160                                feature_fitness    0.000031\n",
      "161                                 feature_dryer_    0.000031\n",
      "162                        feature_laundry_in_unit    0.000031\n",
      "\n",
      "[163 rows x 2 columns]\n",
      "Wall time: 50.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = xgb.XGBClassifier(learning_rate = 0.1\n",
    "                  , n_estimators =246\n",
    "                  , max_depth = 6\n",
    "                  , min_child_weight = 1\n",
    "                  , subsample = 0.7\n",
    "                  , colsample_bytree = 0.7\n",
    "                  , gamma = 1\n",
    "                  , seed = 1234\n",
    "                  , nthread = -1\n",
    "                  )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "clf.fit(train_x, train_y)\n",
    "\n",
    "feature_importance = pd.DataFrame(sorted(zip(full_vars,clf.feature_importances_)\n",
    "                          , key=lambda x: x[1], reverse = True),columns=['feature_name','importance']) \n",
    "\n",
    "print (feature_importance.query('importance>0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "feature_importance.to_csv(r'C:\\Users\\Eric Yang\\Desktop\\Kaggle Comptetion\\Current Competition\\Predictions\\f.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Manuanl Tuning\n",
    "\n",
    "* Greedy-search\n",
    "* Tune one parameter a time\n",
    "* The results can be used for further tuning (by Bayesian Optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "xgb_scores = pd.DataFrame()\n",
    "scores = []\n",
    "for max_depth in [3,4,5,6,7,8,9,10]:\n",
    "\n",
    "    params = dict()\n",
    "    params['objective'] = 'multi:softprob'\n",
    "    params['num_class'] = 3\n",
    "    params['eta'] = 0.1\n",
    "    params['max_depth'] = max_depth\n",
    "    params['min_child_weight'] = 1\n",
    "    params['subsample'] = 1\n",
    "    params['colsample_bytree'] = 1\n",
    "    params['gamma'] = 0\n",
    "    params['seed']=1234\n",
    "\n",
    "    cv_results = xgb.cv(params, xgb.DMatrix(train_x, label=train_y.reshape(train_x.shape[0],1)),\n",
    "                   num_boost_round=1000000,\n",
    "                   nfold=5,\n",
    "           metrics={'mlogloss'},\n",
    "           seed=1234,\n",
    "           callbacks=[xgb.callback.early_stop(50)])\n",
    "    best_iteration = len(cv_results)\n",
    "    best_score = cv_results['test-mlogloss-mean'].min()\n",
    "    print (max_depth,best_score,best_iteration)\n",
    "    scores.append([best_score,params['eta'],params['max_depth'],params['min_child_weight'],\n",
    "                      params['colsample_bytree'],params['subsample'],params['gamma'],best_iteration])\n",
    "xgb_scores = pd.concat([xgb_scores, pd.DataFrame(scores,columns=['score','eta','max_depth','min_child_weight',\n",
    "                                   'colsample_bytree','subsample','gamma','best_iteration'])])    \n",
    "best_max_depth = int(pd.DataFrame(scores,columns=['score','eta','max_depth','min_child_weight',\n",
    "                                   'colsample_bytree','subsample','gamma','best_iteration']).sort_values(by='score',ascending=True)['max_depth'].values[0])\n",
    "print ('best max_depth is', best_max_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "scores = []\n",
    "for min_child_weight in [1,3,10,30,100]:\n",
    "\n",
    "    params = dict()\n",
    "    params['objective'] = 'multi:softprob'\n",
    "    params['num_class'] = 3\n",
    "    params['eta'] = 0.1\n",
    "    params['max_depth'] = best_max_depth\n",
    "    params['min_child_weight'] = min_child_weight\n",
    "    params['subsample'] = 1\n",
    "    params['colsample_bytree'] = 1\n",
    "    params['gamma'] = 0\n",
    "    params['seed']=1234\n",
    "\n",
    "    cv_results = xgb.cv(params, xgb.DMatrix(train_x, label=train_y.reshape(train_x.shape[0],1)),\n",
    "                   num_boost_round=1000000,\n",
    "                   nfold=5,\n",
    "           metrics={'mlogloss'},\n",
    "           seed=1234,\n",
    "           callbacks=[xgb.callback.early_stop(50)])\n",
    "    best_iteration = len(cv_results)\n",
    "    best_score = cv_results['test-mlogloss-mean'].min()\n",
    "    print (min_child_weight,best_score,best_iteration)\n",
    "    scores.append([best_score,params['eta'],params['max_depth'],params['min_child_weight'],\n",
    "                      params['colsample_bytree'],params['subsample'],params['gamma'],best_iteration])\n",
    "xgb_scores = pd.concat([xgb_scores, pd.DataFrame(scores,columns=['score','eta','max_depth','min_child_weight',\n",
    "                                   'colsample_bytree','subsample','gamma','best_iteration'])])    \n",
    "best_min_child_weight = int(pd.DataFrame(scores,columns=['score','eta','max_depth','min_child_weight',\n",
    "                                   'colsample_bytree','subsample','gamma','best_iteration']).sort_values(by='score',ascending=True)['min_child_weight'].values[0])\n",
    "print ('best min_child_weight is', best_min_child_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "scores = []\n",
    "for colsample_bytree in [0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]:\n",
    "\n",
    "    params = dict()\n",
    "    params['objective'] = 'multi:softprob'\n",
    "    params['num_class'] = 3\n",
    "    params['eta'] = 0.1\n",
    "    params['max_depth'] = best_max_depth\n",
    "    params['min_child_weight'] = best_min_child_weight\n",
    "    params['subsample'] = 1\n",
    "    params['colsample_bytree'] = colsample_bytree\n",
    "    params['gamma'] = 0\n",
    "    params['seed']=1234\n",
    "\n",
    "    cv_results = xgb.cv(params, xgb.DMatrix(train_x, label=train_y.reshape(train_x.shape[0],1)),\n",
    "                   num_boost_round=1000000,\n",
    "                   nfold=5,\n",
    "           metrics={'mlogloss'},\n",
    "           seed=1234,\n",
    "           callbacks=[xgb.callback.early_stop(50)])\n",
    "    best_iteration = len(cv_results)\n",
    "    best_score = cv_results['test-mlogloss-mean'].min()\n",
    "    print (colsample_bytree,best_score,best_iteration)\n",
    "    scores.append([best_score,params['eta'],params['max_depth'],params['min_child_weight'],\n",
    "                      params['colsample_bytree'],params['subsample'],params['gamma'],best_iteration])\n",
    "xgb_scores = pd.concat([xgb_scores, pd.DataFrame(scores,columns=['score','eta','max_depth','min_child_weight',\n",
    "                                   'colsample_bytree','subsample','gamma','best_iteration'])])    \n",
    "best_colsample_bytree = pd.DataFrame(scores,columns=['score','eta','max_depth','min_child_weight',\n",
    "                                   'colsample_bytree','subsample','gamma','best_iteration']).sort_values(by='score',ascending=True)['colsample_bytree'].values[0]\n",
    "\n",
    "\n",
    "print ('best colsample_bytree is', best_colsample_bytree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "scores = []\n",
    "for subsample in [0.6,0.7,0.8,0.9,1]:\n",
    "\n",
    "    params = dict()\n",
    "    params['objective'] = 'multi:softprob'\n",
    "    params['num_class'] = 3\n",
    "    params['eta'] = 0.1\n",
    "    params['max_depth'] = best_max_depth\n",
    "    params['min_child_weight'] = best_min_child_weight\n",
    "    params['subsample'] = subsample\n",
    "    params['colsample_bytree'] = best_colsample_bytree\n",
    "    params['gamma'] = 0\n",
    "    params['seed']=1234\n",
    "\n",
    "    cv_results = xgb.cv(params, xgb.DMatrix(train_x, label=train_y.reshape(train_x.shape[0],1)),\n",
    "                   num_boost_round=1000000,\n",
    "                   nfold=5,\n",
    "           metrics={'mlogloss'},\n",
    "           seed=1234,\n",
    "           callbacks=[xgb.callback.early_stop(50)])\n",
    "    best_iteration = len(cv_results)\n",
    "    best_score = cv_results['test-mlogloss-mean'].min()\n",
    "    print (subsample,best_score,best_iteration)\n",
    "    scores.append([best_score,params['eta'],params['max_depth'],params['min_child_weight'],\n",
    "                      params['colsample_bytree'],params['subsample'],params['gamma'],best_iteration])\n",
    "xgb_scores = pd.concat([xgb_scores, pd.DataFrame(scores,columns=['score','eta','max_depth','min_child_weight',\n",
    "                                   'colsample_bytree','subsample','gamma','best_iteration'])])    \n",
    "best_subsample = pd.DataFrame(scores,columns=['score','eta','max_depth','min_child_weight',\n",
    "                                   'colsample_bytree','subsample','gamma','best_iteration']).sort_values(by='score',ascending=True)['subsample'].values[0]\n",
    "\n",
    "print ('best subsample is', best_subsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "scores = []\n",
    "for gamma in [0,0.5,1,1.5,2]:\n",
    "\n",
    "    params = dict()\n",
    "    params['objective'] = 'multi:softprob'\n",
    "    params['num_class'] = 3\n",
    "    params['eta'] = 0.1\n",
    "    params['max_depth'] = best_max_depth\n",
    "    params['min_child_weight'] = best_min_child_weight\n",
    "    params['subsample'] = best_subsample\n",
    "    params['colsample_bytree'] = best_colsample_bytree\n",
    "    params['gamma'] = gamma\n",
    "    params['seed']=1234\n",
    "\n",
    "    cv_results = xgb.cv(params, xgb.DMatrix(train_x, label=train_y.reshape(train_x.shape[0],1)),\n",
    "                   num_boost_round=1000000,\n",
    "                   nfold=5,\n",
    "           metrics={'mlogloss'},\n",
    "           seed=1234,\n",
    "           callbacks=[xgb.callback.early_stop(50)])\n",
    "    best_iteration = len(cv_results)\n",
    "    best_score = cv_results['test-mlogloss-mean'].min()\n",
    "    print (gamma,best_score,best_iteration)\n",
    "    scores.append([best_score,params['eta'],params['max_depth'],params['min_child_weight'],\n",
    "                      params['colsample_bytree'],params['subsample'],params['gamma'],best_iteration])\n",
    "xgb_scores = pd.concat([xgb_scores, pd.DataFrame(scores,columns=['score','eta','max_depth','min_child_weight',\n",
    "                                   'colsample_bytree','subsample','gamma','best_iteration'])])    \n",
    "best_gamma = pd.DataFrame(scores,columns=['score','eta','max_depth','min_child_weight',\n",
    "                                   'colsample_bytree','subsample','gamma','best_iteration']).sort_values(by='score',ascending=True)['gamma'].values[0]\n",
    "\n",
    "print ('best gamma is', best_gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Automated Tuning\n",
    "\n",
    "* https://github.com/fmfn/BayesianOptimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInitialization\u001b[0m\n",
      "\u001b[94m---------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   colsample_bytree |     gamma |   max_depth |   min_child_weight |   subsample | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[169]\ttrain-mlogloss:0.33857+0.0023279\ttest-mlogloss:0.529338+0.0060731\n",
      "\n",
      "    1 | 03m35s | \u001b[35m  -0.52934\u001b[0m | \u001b[32m            0.6283\u001b[0m | \u001b[32m   1.2582\u001b[0m | \u001b[32m     8.6441\u001b[0m | \u001b[32m           19.4144\u001b[0m | \u001b[32m     0.8109\u001b[0m | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[791]\ttrain-mlogloss:0.415727+0.00195773\ttest-mlogloss:0.529571+0.00682249\n",
      "\n",
      "    2 | 04m24s |   -0.52957 |             0.2592 |    1.0328 |      4.0234 |            84.7801 |      0.9292 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[401]\ttrain-mlogloss:0.388403+0.00151615\ttest-mlogloss:0.528965+0.00622884\n",
      "\n",
      "    3 | 06m12s | \u001b[35m  -0.52897\u001b[0m | \u001b[32m            0.7662\u001b[0m | \u001b[32m   0.8058\u001b[0m | \u001b[32m     6.8024\u001b[0m | \u001b[32m           95.7146\u001b[0m | \u001b[32m     0.9063\u001b[0m | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[139]\ttrain-mlogloss:0.286108+0.00222948\ttest-mlogloss:0.531551+0.00605031\n",
      "\n",
      "    4 | 02m12s |   -0.53155 |             0.3080 |    0.5426 |      9.9217 |             6.2199 |      0.7298 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[648]\ttrain-mlogloss:0.38158+0.00121205\ttest-mlogloss:0.528323+0.00597318\n",
      "\n",
      "    5 | 05m27s | \u001b[35m  -0.52832\u001b[0m | \u001b[32m            0.5416\u001b[0m | \u001b[32m   1.8168\u001b[0m | \u001b[32m     4.3853\u001b[0m | \u001b[32m            6.5088\u001b[0m | \u001b[32m     0.7982\u001b[0m | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[323]\ttrain-mlogloss:0.339562+0.00184995\ttest-mlogloss:0.5277+0.00612426\n",
      "\n",
      "    6 | 04m16s | \u001b[35m  -0.52770\u001b[0m | \u001b[32m            0.4641\u001b[0m | \u001b[32m   1.2286\u001b[0m | \u001b[32m     7.8159\u001b[0m | \u001b[32m           38.4234\u001b[0m | \u001b[32m     0.9087\u001b[0m | \n",
      "\u001b[31mBayesian Optimization\u001b[0m\n",
      "\u001b[94m---------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   colsample_bytree |     gamma |   max_depth |   min_child_weight |   subsample | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[899]\ttrain-mlogloss:0.42749+0.00193472\ttest-mlogloss:0.530129+0.00646387\n",
      "\n",
      "    7 | 07m12s |   -0.53013 |             0.6985 |    0.0326 |      3.0061 |            35.4836 |      0.7465 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[219]\ttrain-mlogloss:0.360917+0.00169127\ttest-mlogloss:0.528704+0.00570651\n",
      "\n",
      "    8 | 05m32s |   -0.52870 |             0.7907 |    1.9885 |      9.8102 |            63.0806 |      0.7855 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1490]\ttrain-mlogloss:0.437977+0.00173458\ttest-mlogloss:0.532261+0.00689124\n",
      "\n",
      "    9 | 05m48s |   -0.53226 |             0.1695 |    1.9129 |      3.2297 |            99.6316 |      0.7467 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[356]\ttrain-mlogloss:0.366217+0.00167569\ttest-mlogloss:0.530491+0.00611696\n",
      "\n",
      "   10 | 03m58s |   -0.53049 |             0.2674 |    1.9345 |      9.9946 |            88.8210 |      0.8047 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[220]\ttrain-mlogloss:0.326426+0.00167098\ttest-mlogloss:0.528977+0.00639499\n",
      "\n",
      "   11 | 05m49s |   -0.52898 |             0.7826 |    0.0004 |      9.5250 |            48.4124 |      0.9753 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[694]\ttrain-mlogloss:0.384644+0.00168862\ttest-mlogloss:0.529261+0.0067417\n",
      "\n",
      "   12 | 07m18s |   -0.52926 |             0.7261 |    1.9179 |      4.1919 |             0.1755 |      0.9751 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[888]\ttrain-mlogloss:0.452426+0.00247865\ttest-mlogloss:0.531457+0.00598295\n",
      "\n",
      "   13 | 07m27s |   -0.53146 |             0.7648 |    1.9824 |      3.1286 |            54.3021 |      0.9949 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[277]\ttrain-mlogloss:0.338357+0.00185223\ttest-mlogloss:0.527975+0.00619531\n",
      "\n",
      "   14 | 06m34s |   -0.52797 |             0.7516 |    0.0182 |      9.5055 |            74.6064 |      0.9721 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[184]\ttrain-mlogloss:0.330935+0.00274961\ttest-mlogloss:0.529221+0.0052951\n",
      "\n",
      "   15 | 05m01s |   -0.52922 |             0.7839 |    1.9983 |      9.6030 |            32.1050 |      0.8471 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1138]\ttrain-mlogloss:0.41601+0.00207681\ttest-mlogloss:0.529649+0.0065755\n",
      "\n",
      "   16 | 05m08s |   -0.52965 |             0.2283 |    0.0173 |      3.0904 |            14.0160 |      0.9441 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[308]\ttrain-mlogloss:0.356194+0.0012526\ttest-mlogloss:0.530273+0.00598367\n",
      "\n",
      "   17 | 05m55s |   -0.53027 |             0.6214 |    0.0898 |      9.7982 |            99.8178 |      0.8086 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[223]\ttrain-mlogloss:0.329303+0.00146938\ttest-mlogloss:0.529182+0.0062036\n",
      "\n",
      "   18 | 05m32s |   -0.52918 |             0.7576 |    1.8877 |      9.9343 |            42.4559 |      0.7586 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1492]\ttrain-mlogloss:0.426427+0.00170111\ttest-mlogloss:0.531583+0.00648097\n",
      "\n",
      "   19 | 05m22s |   -0.53158 |             0.1072 |    0.0405 |      3.2651 |            66.5158 |      0.9217 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[313]\ttrain-mlogloss:0.326408+0.00221487\ttest-mlogloss:0.527964+0.00658624\n",
      "\n",
      "   20 | 03m11s |   -0.52796 |             0.1922 |    0.0994 |      9.9372 |            57.8858 |      0.9457 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1786]\ttrain-mlogloss:0.429022+0.00476108\ttest-mlogloss:0.529209+0.00609497\n",
      "\n",
      "   21 | 06m59s |   -0.52921 |             0.1542 |    1.9979 |      3.0170 |            20.8095 |      0.9937 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[691]\ttrain-mlogloss:0.413845+0.00166461\ttest-mlogloss:0.5291+0.0068189\n",
      "\n",
      "   22 | 07m30s |   -0.52910 |             0.7780 |    0.0144 |      4.6980 |            90.8521 |      0.9814 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[339]\ttrain-mlogloss:0.340231+0.00195832\ttest-mlogloss:0.528442+0.00638873\n",
      "\n",
      "   23 | 03m08s |   -0.52844 |             0.1560 |    0.0250 |      9.7795 |            69.2511 |      0.9948 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[193]\ttrain-mlogloss:0.324907+0.00271384\ttest-mlogloss:0.528006+0.00565361\n",
      "\n",
      "   24 | 04m42s |   -0.52801 |             0.6733 |    0.1363 |      9.8859 |            37.6532 |      0.9868 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[453]\ttrain-mlogloss:0.370569+0.00185479\ttest-mlogloss:0.529509+0.00608883\n",
      "\n",
      "   25 | 06m20s |   -0.52951 |             0.7699 |    1.9377 |      5.5627 |            10.7138 |      0.9986 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1238]\ttrain-mlogloss:0.420097+0.00202357\ttest-mlogloss:0.531028+0.006315\n",
      "\n",
      "   26 | 04m32s |   -0.53103 |             0.1049 |    0.3384 |      3.0252 |             4.2019 |      0.9703 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[400]\ttrain-mlogloss:0.359406+0.00110482\ttest-mlogloss:0.533796+0.00615951\n",
      "\n",
      "   27 | 03m06s |   -0.53380 |             0.1205 |    0.0420 |      9.6123 |            79.5009 |      0.7025 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1188]\ttrain-mlogloss:0.424843+0.0018385\ttest-mlogloss:0.530307+0.00637621\n",
      "\n",
      "   28 | 09m07s |   -0.53031 |             0.7031 |    1.9810 |      3.2299 |            75.1515 |      0.8784 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[152]\ttrain-mlogloss:0.25259+0.0013756\ttest-mlogloss:0.531871+0.00565265\n",
      "\n",
      "   29 | 04m19s |   -0.53187 |             0.6861 |    1.6888 |      9.8631 |             0.0721 |      0.9220 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[247]\ttrain-mlogloss:0.352947+0.00131207\ttest-mlogloss:0.528703+0.00539321\n",
      "\n",
      "   30 | 06m02s |   -0.52870 |             0.7663 |    1.9865 |      9.9469 |            71.1774 |      0.8842 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[200]\ttrain-mlogloss:0.366123+0.00218266\ttest-mlogloss:0.530008+0.00515467\n",
      "\n",
      "   31 | 05m06s |   -0.53001 |             0.7842 |    0.0097 |      9.8901 |            60.1102 |      0.7048 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[441]\ttrain-mlogloss:0.350059+0.00117764\ttest-mlogloss:0.53078+0.00592398\n",
      "\n",
      "   32 | 03m31s |   -0.53078 |             0.1114 |    1.9110 |      9.8750 |            53.3450 |      0.8950 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1186]\ttrain-mlogloss:0.430839+0.00182068\ttest-mlogloss:0.530789+0.00692953\n",
      "\n",
      "   33 | 04m44s |   -0.53079 |             0.1497 |    0.0996 |      3.2228 |            44.0804 |      0.9843 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[346]\ttrain-mlogloss:0.351972+0.00304087\ttest-mlogloss:0.528924+0.0060365\n",
      "\n",
      "   34 | 05m31s |   -0.52892 |             0.7031 |    0.0340 |      6.3400 |            27.0908 |      0.9930 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[271]\ttrain-mlogloss:0.366402+0.00126468\ttest-mlogloss:0.528954+0.0060537\n",
      "\n",
      "   35 | 06m35s |   -0.52895 |             0.7709 |    1.8871 |      9.8425 |            94.4468 |      0.9936 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[405]\ttrain-mlogloss:0.305392+0.00133427\ttest-mlogloss:0.528943+0.00676338\n",
      "\n",
      "   36 | 03m34s |   -0.52894 |             0.1336 |    1.9109 |      9.7459 |            23.1222 |      0.9671 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[927]\ttrain-mlogloss:0.401923+0.00226101\ttest-mlogloss:0.527648+0.00673797\n",
      "\n",
      "   37 | 05m04s | \u001b[35m  -0.52765\u001b[0m | \u001b[32m            0.2227\u001b[0m | \u001b[32m   1.9367\u001b[0m | \u001b[32m     4.6222\u001b[0m | \u001b[32m           32.3823\u001b[0m | \u001b[32m     0.9892\u001b[0m | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1124]\ttrain-mlogloss:0.44796+0.00310699\ttest-mlogloss:0.531108+0.00614841\n",
      "\n",
      "   38 | 09m17s |   -0.53111 |             0.7645 |    1.9025 |      3.6784 |            38.5615 |      0.9985 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[415]\ttrain-mlogloss:0.336445+0.00175771\ttest-mlogloss:0.529849+0.00772474\n",
      "\n",
      "   39 | 02m55s |   -0.52985 |             0.1105 |    0.1419 |      7.7463 |            33.6881 |      0.9313 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1082]\ttrain-mlogloss:0.417073+0.00200916\ttest-mlogloss:0.529642+0.00616373\n",
      "\n",
      "   40 | 09m11s |   -0.52964 |             0.7420 |    1.9988 |      3.1290 |            27.7571 |      0.9230 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1327]\ttrain-mlogloss:0.441838+0.00195267\ttest-mlogloss:0.530253+0.00628939\n",
      "\n",
      "   41 | 06m28s |   -0.53025 |             0.2897 |    1.9421 |      3.0036 |            91.6529 |      0.9514 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[279]\ttrain-mlogloss:0.339321+0.00235815\ttest-mlogloss:0.527184+0.00665982\n",
      "\n",
      "   42 | 02m58s | \u001b[35m  -0.52718\u001b[0m | \u001b[32m            0.2237\u001b[0m | \u001b[32m   0.0542\u001b[0m | \u001b[32m     8.7044\u001b[0m | \u001b[32m           41.6280\u001b[0m | \u001b[32m     0.9894\u001b[0m | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[407]\ttrain-mlogloss:0.294938+0.00171696\ttest-mlogloss:0.528418+0.00746596\n",
      "\n",
      "   43 | 03m38s |   -0.52842 |             0.1333 |    1.9980 |      9.6908 |            15.6542 |      0.9763 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[382]\ttrain-mlogloss:0.320877+0.00141345\ttest-mlogloss:0.527944+0.00692691\n",
      "\n",
      "   44 | 03m40s |   -0.52794 |             0.1676 |    1.8912 |      9.7079 |            38.4174 |      0.9604 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[425]\ttrain-mlogloss:0.368809+0.00119859\ttest-mlogloss:0.528595+0.00620198\n",
      "\n",
      "   45 | 07m01s |   -0.52859 |             0.7864 |    0.0652 |      6.5722 |            71.9607 |      0.9805 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[498]\ttrain-mlogloss:0.321283+0.00239646\ttest-mlogloss:0.528335+0.0068165\n",
      "\n",
      "   46 | 03m31s |   -0.52833 |             0.1167 |    1.9802 |      7.7792 |             3.9320 |      0.9902 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[309]\ttrain-mlogloss:0.358145+0.00217536\ttest-mlogloss:0.527591+0.00638231\n",
      "\n",
      "   47 | 04m14s |   -0.52759 |             0.3423 |    0.0355 |      9.9800 |            92.8979 |      0.9940 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[375]\ttrain-mlogloss:0.365592+0.00189916\ttest-mlogloss:0.527031+0.00607703\n",
      "\n",
      "   48 | 04m19s | \u001b[35m  -0.52703\u001b[0m | \u001b[32m            0.3577\u001b[0m | \u001b[32m   1.9175\u001b[0m | \u001b[32m     7.3930\u001b[0m | \u001b[32m           59.6540\u001b[0m | \u001b[32m     0.9887\u001b[0m | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[572]\ttrain-mlogloss:0.363778+0.00182161\ttest-mlogloss:0.529049+0.00640289\n",
      "\n",
      "   49 | 03m35s |   -0.52905 |             0.1301 |    0.0688 |      6.5291 |            58.8492 |      0.9941 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[581]\ttrain-mlogloss:0.341966+0.00106379\ttest-mlogloss:0.529592+0.00681774\n",
      "\n",
      "   50 | 04m23s |   -0.52959 |             0.1014 |    1.9146 |      9.3210 |            60.3276 |      0.9897 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1240]\ttrain-mlogloss:0.428577+0.00168518\ttest-mlogloss:0.530579+0.00591434\n",
      "\n",
      "   51 | 10m31s |   -0.53058 |             0.7949 |    1.8495 |      3.5953 |            61.7742 |      0.9811 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[279]\ttrain-mlogloss:0.336018+0.00254303\ttest-mlogloss:0.528609+0.00598016\n",
      "\n",
      "   52 | 06m16s |   -0.52861 |             0.7632 |    0.2253 |      8.6827 |            55.8307 |      0.9863 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1229]\ttrain-mlogloss:0.437664+0.00190121\ttest-mlogloss:0.532231+0.0069126\n",
      "\n",
      "   53 | 04m27s |   -0.53223 |             0.1044 |    1.9865 |      3.0928 |            14.9047 |      0.7190 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[324]\ttrain-mlogloss:0.365303+0.00189148\ttest-mlogloss:0.529093+0.00555503\n",
      "\n",
      "   54 | 06m07s |   -0.52909 |             0.7313 |    1.5782 |      7.4064 |            66.1199 |      0.9935 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1029]\ttrain-mlogloss:0.437271+0.00149873\ttest-mlogloss:0.531881+0.00639317\n",
      "\n",
      "   55 | 08m37s |   -0.53188 |             0.7697 |    0.0808 |      3.0791 |            95.8372 |      0.9997 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[160]\ttrain-mlogloss:0.291869+0.00315232\ttest-mlogloss:0.529607+0.00650033\n",
      "\n",
      "   56 | 04m40s |   -0.52961 |             0.7478 |    0.1617 |      9.9602 |            16.6587 |      0.9891 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[687]\ttrain-mlogloss:0.409667+0.00170159\ttest-mlogloss:0.528814+0.00682832\n",
      "\n",
      "   57 | 03m36s |   -0.52881 |             0.1595 |    0.1099 |      4.8784 |            22.6033 |      0.9989 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[561]\ttrain-mlogloss:0.364009+0.00104349\ttest-mlogloss:0.530229+0.00695267\n",
      "\n",
      "   58 | 03m46s |   -0.53023 |             0.1184 |    0.3195 |      7.7027 |            93.2618 |      0.9935 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[273]\ttrain-mlogloss:0.355202+0.00170051\ttest-mlogloss:0.528827+0.00576741\n",
      "\n",
      "   59 | 06m44s |   -0.52883 |             0.7751 |    0.1600 |      9.7862 |            90.8835 |      0.9849 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1047]\ttrain-mlogloss:0.431529+0.00198407\ttest-mlogloss:0.531213+0.00626506\n",
      "\n",
      "   60 | 07m51s |   -0.53121 |             0.6625 |    0.1008 |      3.0136 |            87.4408 |      0.7232 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1022]\ttrain-mlogloss:0.439439+0.00167182\ttest-mlogloss:0.530429+0.00640001\n",
      "\n",
      "   61 | 08m49s |   -0.53043 |             0.7961 |    1.6402 |      3.0526 |            80.2413 |      0.9747 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1259]\ttrain-mlogloss:0.425513+0.00169278\ttest-mlogloss:0.53074+0.00697786\n",
      "\n",
      "   62 | 04m53s |   -0.53074 |             0.1264 |    0.0361 |      3.1889 |            31.4790 |      0.9969 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[657]\ttrain-mlogloss:0.357774+0.00161107\ttest-mlogloss:0.527261+0.00751563\n",
      "\n",
      "   63 | 04m19s |   -0.52726 |             0.1480 |    1.9816 |      6.6251 |            29.0310 |      0.9959 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[368]\ttrain-mlogloss:0.354298+0.0028119\ttest-mlogloss:0.528173+0.00632377\n",
      "\n",
      "   64 | 05m29s |   -0.52817 |             0.6284 |    1.9477 |      6.6618 |            25.0704 |      0.9664 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[411]\ttrain-mlogloss:0.386511+0.00211816\ttest-mlogloss:0.528202+0.00690962\n",
      "\n",
      "   65 | 06m45s |   -0.52820 |             0.7625 |    1.9930 |      6.3796 |            86.0371 |      0.9537 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[318]\ttrain-mlogloss:0.335345+0.00273249\ttest-mlogloss:0.52799+0.00612181\n",
      "\n",
      "   66 | 05m44s |   -0.52799 |             0.7783 |    1.9492 |      6.1618 |             4.7920 |      0.9432 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[310]\ttrain-mlogloss:0.36484+0.00181087\ttest-mlogloss:0.528397+0.00636966\n",
      "\n",
      "   67 | 05m51s |   -0.52840 |             0.7185 |    1.9757 |      7.0302 |            57.5513 |      0.9401 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[214]\ttrain-mlogloss:0.311808+0.00250704\ttest-mlogloss:0.52752+0.00659069\n",
      "\n",
      "   68 | 04m59s |   -0.52752 |             0.6102 |    1.9846 |      9.9037 |            27.9244 |      0.9606 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[253]\ttrain-mlogloss:0.357453+0.0025537\ttest-mlogloss:0.528446+0.00619364\n",
      "\n",
      "   69 | 05m24s |   -0.52845 |             0.7699 |    0.0631 |      7.7335 |            39.9038 |      0.9996 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[304]\ttrain-mlogloss:0.315629+0.00190999\ttest-mlogloss:0.52882+0.00625625\n",
      "\n",
      "   70 | 03m03s |   -0.52882 |             0.1479 |    0.2951 |      9.8780 |            44.1674 |      0.9847 | \n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[621]\ttrain-mlogloss:0.395917+0.00219385\ttest-mlogloss:0.530005+0.00693001\n",
      "\n",
      "   71 | 03m21s |   -0.53001 |             0.1013 |    0.0476 |      5.6589 |            48.6981 |      0.9656 | \n"
     ]
    }
   ],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "xgtrain = xgb.DMatrix(train_x, label=train_y.reshape(train_x.shape[0],1))\n",
    "\n",
    "def xgb_evaluate(min_child_weight,\n",
    "                 colsample_bytree,\n",
    "                 max_depth,\n",
    "                 subsample,\n",
    "                 gamma):\n",
    "    params = dict()\n",
    "    params['objective'] = 'multi:softprob'\n",
    "    params['num_class'] = 3\n",
    "    params['eta'] = 0.1\n",
    "    params['max_depth'] = int(max_depth )   \n",
    "    params['min_child_weight'] = int(min_child_weight)\n",
    "    params['colsample_bytree'] = colsample_bytree\n",
    "    params['subsample'] = subsample\n",
    "    params['gamma'] = gamma\n",
    "    params['verbose_eval'] = True    \n",
    "\n",
    "\n",
    "    cv_result = xgb.cv(params, xgtrain,\n",
    "                       num_boost_round=100000,\n",
    "                       nfold=5,\n",
    "                       metrics={'mlogloss'},\n",
    "                       seed=1234,\n",
    "                       callbacks=[xgb.callback.early_stop(50)])\n",
    "\n",
    "    return -cv_result['test-mlogloss-mean'].min()\n",
    "\n",
    "\n",
    "xgb_BO = BayesianOptimization(xgb_evaluate, \n",
    "                             {'max_depth': (6, 9),\n",
    "                              'min_child_weight': (0, 100),\n",
    "                              'colsample_bytree': (0.1, 0.8),\n",
    "                              'subsample': (0.9, 1),\n",
    "                              'gamma': (0, 2)\n",
    "                             }\n",
    "                            )\n",
    "\n",
    "xgb_BO.maximize(init_points=6, n_iter=65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>colsample_bytree</th>\n",
       "      <th>gamma</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>min_child_weight</th>\n",
       "      <th>subsample</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.357692</td>\n",
       "      <td>1.917539</td>\n",
       "      <td>7.393037</td>\n",
       "      <td>59.653996</td>\n",
       "      <td>0.988750</td>\n",
       "      <td>-0.527031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.223662</td>\n",
       "      <td>0.054163</td>\n",
       "      <td>8.704360</td>\n",
       "      <td>41.628036</td>\n",
       "      <td>0.989407</td>\n",
       "      <td>-0.527184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.147966</td>\n",
       "      <td>1.981550</td>\n",
       "      <td>6.625051</td>\n",
       "      <td>29.030979</td>\n",
       "      <td>0.995917</td>\n",
       "      <td>-0.527261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0.610225</td>\n",
       "      <td>1.984551</td>\n",
       "      <td>9.903677</td>\n",
       "      <td>27.924436</td>\n",
       "      <td>0.960611</td>\n",
       "      <td>-0.527520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.342271</td>\n",
       "      <td>0.035525</td>\n",
       "      <td>9.979960</td>\n",
       "      <td>92.897895</td>\n",
       "      <td>0.993977</td>\n",
       "      <td>-0.527591</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    colsample_bytree     gamma  max_depth  min_child_weight  subsample  \\\n",
       "41          0.357692  1.917539   7.393037         59.653996   0.988750   \n",
       "35          0.223662  0.054163   8.704360         41.628036   0.989407   \n",
       "56          0.147966  1.981550   6.625051         29.030979   0.995917   \n",
       "61          0.610225  1.984551   9.903677         27.924436   0.960611   \n",
       "40          0.342271  0.035525   9.979960         92.897895   0.993977   \n",
       "\n",
       "       score  \n",
       "41 -0.527031  \n",
       "35 -0.527184  \n",
       "56 -0.527261  \n",
       "61 -0.527520  \n",
       "40 -0.527591  "
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Show tuning results\n",
    "BO_scores = pd.DataFrame(xgb_BO.res['all']['params'])\n",
    "BO_scores['score'] = pd.DataFrame(xgb_BO.res['all']['values'])\n",
    "BO_scores = BO_scores.sort_values(by='score',ascending=False)\n",
    "\n",
    "BO_scores.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Re-train models\n",
    "\n",
    "Now we have optimized parameters, let's decrease the size of learning rate and train the model for better results.\n",
    "\n",
    "Firstly we'll use xgb.cv again to get optimal n_estimators, then we can use tuned n_esimator to finally train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 500 rounds.\n",
      "Stopping. Best iteration:\n",
      "[4665]\ttrain-mlogloss:0.333136+0.0012015\ttest-mlogloss:0.521276+0.00964687\n",
      "\n"
     ]
    }
   ],
   "source": [
    "params = dict()\n",
    "params['objective'] = 'multi:softprob'\n",
    "params['num_class'] = 3\n",
    "params['eta'] = 0.01\n",
    "params['max_depth'] = int(BO_scores.to_dict()['max_depth'][0])\n",
    "params['min_child_weight'] = BO_scores.to_dict()['min_child_weight'][0]\n",
    "params['colsample_bytree'] = BO_scores.to_dict()['colsample_bytree'][0]\n",
    "params['subsample'] = BO_scores.to_dict()['subsample'][0]\n",
    "params['gamma'] = BO_scores.to_dict()['gamma'][0]\n",
    "params['seed']=1234\n",
    "\n",
    "cv_results = xgb.cv(params, xgb.DMatrix(train_x, label=train_y.reshape(train_x.shape[0],1)),\n",
    "               num_boost_round=1000000, nfold=10,\n",
    "       metrics={'mlogloss'},\n",
    "       seed=1234,\n",
    "       callbacks=[xgb.callback.early_stop(500)])\n",
    "\n",
    "best_iteration = len(cv_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                feature_name  importance\n",
      "0                       hcc_building_id_high    0.031839\n",
      "1                     hcc_building_id_medium    0.031707\n",
      "2                          price_per_bedroom    0.030547\n",
      "3                                      price    0.030518\n",
      "4                        hcc_manager_id_high    0.030303\n",
      "5                price_percentile_by_manager    0.030082\n",
      "6                             price_per_room    0.028758\n",
      "7                             dist_to_center    0.028652\n",
      "8              price_percentile_by_disp_addr    0.027419\n",
      "9                       manager_level_medium    0.026829\n",
      "10                           price_per_photo    0.025932\n",
      "11                              avg_word_len    0.025616\n",
      "12       created_epoch_percentile_by_manager    0.025510\n",
      "13                     hcc_manager_id_medium    0.025508\n",
      "14                                  latitude    0.025462\n",
      "15              price_percentile_by_building    0.025379\n",
      "16                         manager_level_low    0.025069\n",
      "17                            building_id_le    0.024610\n",
      "18                        manager_level_high    0.024129\n",
      "19                                 longitude    0.022971\n",
      "20                        display_address_le    0.022802\n",
      "21                         street_address_le    0.022008\n",
      "22                                listing_id    0.019695\n",
      "23                               len_of_desc    0.018898\n",
      "24                             manager_id_le    0.018746\n",
      "25                        price_per_bathroom    0.018359\n",
      "26                             words_of_desc    0.018290\n",
      "27             mean_created_epoch_by_manager    0.015412\n",
      "28           median_created_epoch_by_manager    0.014911\n",
      "29   median_created_epoch_by_display_address    0.014905\n",
      "..                                       ...         ...\n",
      "104                    feature_live_in_super    0.000158\n",
      "105                                desc_34th    0.000155\n",
      "106                   feature_video_intercom    0.000129\n",
      "107                            created_month    0.000123\n",
      "108                  feature_luxury_building    0.000123\n",
      "109                                 desc_ave    0.000123\n",
      "110      feature_outdoor_entertainment_space    0.000115\n",
      "111                     feature__dishwasher_    0.000103\n",
      "112                                desc_west    0.000089\n",
      "113                                 desc_200    0.000077\n",
      "114                        feature_furnished    0.000075\n",
      "115                  feature_private_terrace    0.000075\n",
      "116                     feature_cats_allowed    0.000063\n",
      "117                     feature_package_room    0.000063\n",
      "118                      feature_unit_washer    0.000043\n",
      "119                      feature_indoor_pool    0.000043\n",
      "120                          feature_simplex    0.000037\n",
      "121                       feature_dishwasher    0.000037\n",
      "122                           feature__dryer    0.000034\n",
      "123                            feature_multi    0.000029\n",
      "124                     feature_site_parking    0.000023\n",
      "125                  feature_wheelchair_ramp    0.000011\n",
      "126                 feature_pets_on_approval    0.000011\n",
      "127                        feature__pets_ok_    0.000009\n",
      "128                             feature_walk    0.000009\n",
      "129                             feature_flex    0.000006\n",
      "130                  feature__exposed_brick_    0.000006\n",
      "131                           feature_dryer_    0.000006\n",
      "132                  feature_gym_in_building    0.000003\n",
      "133                           feature_lounge    0.000003\n",
      "\n",
      "[134 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "clf = xgb.XGBClassifier(n_estimators = best_iteration,\n",
    "                              learning_rate=0.01,\n",
    "                              max_depth=int(BO_scores.to_dict()['max_depth'][0]),\n",
    "                              min_child_weight=BO_scores.to_dict()['min_child_weight'][0],\n",
    "                              colsample_bytree=BO_scores.to_dict()['colsample_bytree'][0],\n",
    "                              subsample=BO_scores.to_dict()['subsample'][0],\n",
    "                              gamma=BO_scores.to_dict()['gamma'][0],\n",
    "                              seed=1234,\n",
    "                              nthread=-1)\n",
    "\n",
    "clf.fit(train_x, train_y)\n",
    "\n",
    "feature_importance = pd.DataFrame(sorted(zip(full_vars,clf.feature_importances_)\n",
    "                          , key=lambda x: x[1], reverse = True),columns=['feature_name','importance']) \n",
    "\n",
    "print (feature_importance.query('importance>0'))\n",
    "\n",
    "\n",
    "preds = clf.predict_proba(test_x)\n",
    "sub_df = pd.DataFrame(preds,columns = [\"low\", \"medium\", \"high\"])\n",
    "sub_df[\"listing_id\"] = test_data.listing_id.values\n",
    "sub_df.to_csv(r\"C:\\Users\\Eric Yang\\Desktop\\Kaggle Comptetion\\Current Competition\\Predictions\\sub_xgb_tuned_v8.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "interest_levels = ['low', 'medium', 'high']\n",
    "\n",
    "tau = {\n",
    "    'low': 0.69195995, \n",
    "    'medium': 0.23108864,\n",
    "    'high': 0.07695141, \n",
    "}\n",
    "\n",
    "def correct(df):\n",
    "    y = df[interest_levels].mean()\n",
    "    a = [tau[k] / y[k]  for k in interest_levels]\n",
    "    print(a)\n",
    "\n",
    "    def f(p):\n",
    "        for k in range(len(interest_levels)):\n",
    "            p[k] *= a[k]\n",
    "        return p / p.sum()\n",
    "\n",
    "    df_correct = df.copy()\n",
    "    df_correct[interest_levels] = df_correct[interest_levels].apply(f, axis=1)\n",
    "\n",
    "    y = df_correct[interest_levels].mean()\n",
    "    a = [tau[k] / y[k]  for k in interest_levels]\n",
    "    print(a)\n",
    "\n",
    "    return df_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.99025408244876045, 1.0199119751777115, 1.0307702227597324]\n",
      "[0.9956691306468165, 1.0076912404379388, 1.0164188334778979]\n"
     ]
    }
   ],
   "source": [
    "sub_df2 = correct(sub_df)\n",
    "sub_df2.to_csv(r\"C:\\Users\\Eric Yang\\Desktop\\Kaggle Comptetion\\Current Competition\\Predictions\\sub_xgb_tuned_v8.1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def blend_model(clfs, train_x, train_y, test_x, num_class, blend_folds):\n",
    "    num_class = 3\n",
    "    blend_folds = 5\n",
    "\n",
    "    skf = model_selection.StratifiedKFold(n_splits=blend_folds,random_state=1234)\n",
    "    skf_ids = list(skf.split(train_x, train_y))\n",
    "\n",
    "\n",
    "    train_blend_x = np.zeros((train_x.shape[0], len(clfs)*num_class))\n",
    "    test_blend_x = np.zeros((test_x.shape[0], len(clfs)*num_class))\n",
    "    blend_scores = np.zeros ((blend_folds,len(clfs)))\n",
    "\n",
    "    print  (\"Start blending.\")\n",
    "    for j, clf in enumerate(clfs):\n",
    "        print (\"Blending model\",j+1, clf)\n",
    "        test_blend_x_j = np.zeros((test_x.shape[0], num_class))\n",
    "        for i, (train_ids, val_ids) in enumerate(skf_ids):\n",
    "            print (\"Model %d fold %d\" %(j+1,i+1))\n",
    "            train_x_fold = train_x[train_ids]\n",
    "            train_y_fold = train_y[train_ids]\n",
    "            val_x_fold = train_x[val_ids]\n",
    "            val_y_fold = train_y[val_ids]\n",
    "            # Set n_estimators to a large number for early_stopping\n",
    "            clf.n_estimators = 10000000\n",
    "            \n",
    "            # Set evaluation metric\n",
    "            if type(clf).__name__=='LGBMClassifier':\n",
    "                metric = 'logloss' #LightGBM\n",
    "            else:\n",
    "                metric = 'mlogloss' #XGBoost            \n",
    "            clf.fit(train_x_fold, train_y_fold,\n",
    "                    eval_set=[(val_x_fold,val_y_fold)],\n",
    "                    eval_metric=metric,\n",
    "                    early_stopping_rounds=500,verbose=False)\n",
    "            val_y_predict_fold = clf.predict_proba(val_x_fold)\n",
    "            score = metrics.log_loss(val_y_fold,val_y_predict_fold)\n",
    "            print (\"LOGLOSS: \", score)\n",
    "            print (\"Best Iteration:\", clf.best_iteration)\n",
    "            blend_scores[i,j]=score\n",
    "            train_blend_x[val_ids, j*num_class:j*num_class+num_class] = val_y_predict_fold\n",
    "            test_blend_x_j = test_blend_x_j + clf.predict_proba(test_x)\n",
    "        test_blend_x[:,j*num_class:j*num_class+num_class] = test_blend_x_j/blend_folds\n",
    "        print (\"Score for model %d is %f\" % (j+1,np.mean(blend_scores[:,j])))\n",
    "    return train_blend_x, test_blend_x, blend_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "xgb_clfs = []\n",
    "for p in BO_scores.head(10).iterrows():\n",
    "    xgb_clfs.append(xgb.XGBClassifier(n_estimators = 10000,\n",
    "                                  learning_rate=0.01,\n",
    "                                  max_depth=int(p[1].to_dict()['max_depth']),\n",
    "                                  min_child_weight=int(p[1].to_dict()['min_child_weight']),\n",
    "                                  colsample_bytree=p[1].to_dict()['colsample_bytree'],\n",
    "                                  subsample=p[1].to_dict()['subsample'],\n",
    "                                  gamma=p[1].to_dict()['gamma'],\n",
    "                                  seed=1234,\n",
    "                                  nthread=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=-1, n_neighbors=5, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_jobs = -1)\n",
    "knn.fit(train_x, label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression,RidgeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "def search_model(train_x, train_y, est, param_grid, n_jobs, cv, refit=False):\n",
    "##Grid Search for the best model\n",
    "    model = model_selection.GridSearchCV(estimator  = est,\n",
    "                                     param_grid = param_grid,\n",
    "                                     scoring    = 'log_loss',\n",
    "                                     verbose    = 10,\n",
    "                                     n_jobs  = n_jobs,\n",
    "                                     iid        = True,\n",
    "                                     refit    = refit,\n",
    "                                     cv      = cv)\n",
    "    # Fit Grid Search Model\n",
    "    model.fit(train_x, train_y)\n",
    "    print(\"Best score: %0.3f\" % model.best_score_)\n",
    "    print(\"Best parameters set:\", model.best_params_)\n",
    "    print(\"Scores:\", model.grid_scores_)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "param_grid = {\"hidden_layer_sizes\":[10,50,100,200,500]\n",
    "              }\n",
    "model = search_model(train_blend_x_xgb\n",
    "                                         , train_y\n",
    "                                         , MLPClassifier()\n",
    "                                         , param_grid\n",
    "                                         , n_jobs=-1\n",
    "                                         , cv=4\n",
    "                                         , refit=True)   \n",
    "\n",
    "print (\"best subsample:\", model.best_params_)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "preds = model.predict_proba(test_blend_x_xgb)\n",
    "sub_df = pd.DataFrame(preds,columns = [\"low\", \"medium\", \"high\"])\n",
    "sub_df[\"listing_id\"] = test_data.listing_id.values\n",
    "sub_df.to_csv(r\"C:\\Users\\Eric Yang\\Desktop\\Kaggle Comptetion\\Current Competition\\Predictions/sub_L1xgb_L2MLP_blended_only_v3.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0023077794348174, 0.99300098174723594, 1.0004625759030992]\n",
      "[1.0007597967418285, 0.99842529091410981, 0.99791371203792278]\n"
     ]
    }
   ],
   "source": [
    "sub_df2 = correct(sub_df)\n",
    "sub_df2.to_csv(r\"C:\\Users\\Eric Yang\\Desktop\\Kaggle Comptetion\\Current Competition\\Predictions\\sub_L1xgb_L2MLP_blended_only_v1.1.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:snakes]",
   "language": "python",
   "name": "conda-env-snakes-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
